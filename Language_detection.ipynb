{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "97pSHf4dUVP7",
        "siUv7DDbV3M6",
        "DZ06YhW7WdWo",
        "DyQWgoVeZT31",
        "vzY819rrat5g"
      ],
      "mount_file_id": "1kE1S0QMeUb8RburaxvClBu2dyWgHVF_l",
      "authorship_tag": "ABX9TyNVUFYhk+B1efcTd1f+xuEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassen8/Language_Detection/blob/main/Language_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Detection (Oromo, Somali, Swahili)\n"
      ],
      "metadata": {
        "id": "97pSHf4dUVP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu45XFLb-lwl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading from the large datasets"
      ],
      "metadata": {
        "id": "siUv7DDbV3M6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the original datasets are very huge, we'll take only the first 20 bytes as that'll give us enough data"
      ],
      "metadata": {
        "id": "3ci4n-X2WFGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the text files for reading\n",
        "with open(\"/content/drive/MyDrive/Language Detection/sw.txt\", \"r\") as f:\n",
        "    # Read the first 20 MB of the file\n",
        "    swahili = f.read(20 * 1024 * 1024)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Language Detection/om.txt\", \"r\") as f:\n",
        "    # Read the first 10 MB of the file\n",
        "    oromo = f.read(20 * 1024 * 1024)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Language Detection/so.txt\", \"r\") as f:\n",
        "    # Read the first 10 MB of the file\n",
        "    somali = f.read(20 * 1024 * 1024)"
      ],
      "metadata": {
        "id": "cvJmxjGm88co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text cleaning and tokenization"
      ],
      "metadata": {
        "id": "DZ06YhW7WdWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeToSentences(text):\n",
        "      sentenceEndings = r'\\?|\\!|\\.'\n",
        "      sentences = re.split(sentenceEndings, text)\n",
        "      sentences = list(filter(None, sentences))\n",
        "      return sentences"
      ],
      "metadata": {
        "id": "Z13u8CTpFhpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(text):        \n",
        "      # Only keeps alphabet characters and removes everything else\n",
        "      text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "      text = re.sub(r'[\\r\\n]', '', text)\n",
        "      return text.lower()"
      ],
      "metadata": {
        "id": "6KLTOQIfFeOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we tokenize the sentences and place them in Dataframes."
      ],
      "metadata": {
        "id": "jPQYHMkpXE4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "om_sentences = tokenizeToSentences(oromo)\n",
        "om_sentences = [sent for sent in om_sentences if len(sent) > 100]\n",
        "df = pd.DataFrame(om_sentences, columns=['text'])\n",
        "df['text'] = df['text'].apply(lambda x: cleanText(x))\n",
        "df = df.assign(language = 'oromo')"
      ],
      "metadata": {
        "id": "Qp2Nj_cnSdAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw_sentences = tokenizeToSentences(swahili)\n",
        "sw_sentences = [sent for sent in sw_sentences if len(sent) > 100]\n",
        "df2 = pd.DataFrame(sw_sentences, columns=['text'])\n",
        "df2['text'] = df2['text'].apply(lambda x: cleanText(x))\n",
        "df2 = df2.assign(language = 'swahili')"
      ],
      "metadata": {
        "id": "__faC8a9H_TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so_sentences = tokenizeToSentences(somali)\n",
        "so_sentences = [sent for sent in so_sentences if len(sent) > 100]\n",
        "df3 = pd.DataFrame(so_sentences, columns=['text'])\n",
        "df3['text'] = df3['text'].apply(lambda x: cleanText(x))\n",
        "df3 = df3.assign(language = 'somali')"
      ],
      "metadata": {
        "id": "pgDfaeaSME68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate all dataframes into one"
      ],
      "metadata": {
        "id": "J0TrsFJjZGi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.append([df2,df3], ignore_index=True)"
      ],
      "metadata": {
        "id": "4NFbI5PhMT4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data inspection and insight"
      ],
      "metadata": {
        "id": "td9ZKDcWZ2L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "5F2uNLU4ngrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.language.value_counts()"
      ],
      "metadata": {
        "id": "HTpIX7F4oSpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[dataset.language =='oromo'].sample(50)"
      ],
      "metadata": {
        "id": "i6ekoz-zokrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "DyQWgoVeZT31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = dataset['text']"
      ],
      "metadata": {
        "id": "8Mm0Kw7ZuFOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=dataset['language']"
      ],
      "metadata": {
        "id": "gZTjiztxpLQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into test and training\n"
      ],
      "metadata": {
        "id": "lHLXRJEKaN_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=100)"
      ],
      "metadata": {
        "id": "9lwkrgOCo9Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,4), analyzer='char')"
      ],
      "metadata": {
        "id": "-Q6y2_kJprQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ],
      "metadata": {
        "id": "oKWWr7-8p03X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "FeUqTwLztJfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the accuracy of the model"
      ],
      "metadata": {
        "id": "O_9mT4Q6aeAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(\"Accuracy is :\",accuracy)"
      ],
      "metadata": {
        "id": "sNsbcJYgxnmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying out the model"
      ],
      "metadata": {
        "id": "vzY819rrat5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "zPORDjnox_7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    lang = model.predict([text])\n",
        "    print(\"Language is \", lang[0])"
      ],
      "metadata": {
        "id": "aUeAZORFyyZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Nilienda shule leo, na nilicheza mpira wa miguu\"\n",
        "predict(text)"
      ],
      "metadata": {
        "id": "f_i1yWCDy_31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G0x3DNG95WC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}